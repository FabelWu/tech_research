#中文只展示黑方块问号
##问题1
* 如中文“号码一致”四个字却显示为“������������”
- [类似问题1](https://stackoverflow.com/questions/68033489/how-to-solve-encoding-problem-in-pyspark-dataframe)
- [类似问题2](https://stackoverflow.com/questions/67018672/pyspark-unicodeencodeerror-ascii-codec-cant-encode-character-ufffd-in-p)

###原因及解法
* 方块问号是unicode字符：\ufffd
* 显示出问题，是因为文件系统的默认编码是ascii，而python3默认是utf-8，此时要么修改文件系统的编码格式，如果不想改，pyspark场景下还可以修改配置

~~~
config = SparkConf()
config.set("spark.driver.extraJavaOptions", "-Dfile.encoding=UTF-8")
config.set("spark.executor.extraJavaOptions", "-Dfile.encoding=UTF-8")
~~~

##问题2
* 通过pyspark#dataframe.show()展示时报 'ascii' codec can't encode character '\ufffd' in position 124: ordinal not in range(128)

###原因及解法
* 加入下面的方法定位，一般defaultencoding都是utf-8，而如果filesystemencoding是ascii则中文显示会有问题

~~~
print(f"defaultencoding--{sys.getdefaultencoding()}")
print(f"filesystemencoding--{sys.getfilesystemencoding()}")
~~~
* python代码中可以加上下面的代码

~~~
import sys
import codecs
sys.stdout=codecs.getwriter("utf-8")(sys.stdout.detach())
~~~
* 如果是pyspark场景下还需要加上如下配置

~~~
config = SparkConf()
config.set("spark.driver.extraJavaOptions", "-Dfile.encoding=UTF-8")
config.set("spark.executor.extraJavaOptions", "-Dfile.encoding=UTF-8")
~~~
* [类似问题](https://stackoverflow.com/questions/9942594/unicodeencodeerror-ascii-codec-cant-encode-character-u-xa0-in-position-20)
* [问题定位参考](https://blog.csdn.net/whatday/article/details/107086244)
* **[解法参考：pyspark sql读取hive表中文乱码](https://juejin.cn/s/pyspark%20sql%E8%AF%BB%E5%8F%96hive%E8%A1%A8%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81)**
* **[解法参考：PySpark 关闭Pyspark中的强制UTF-8编码](https://geek-docs.com/pyspark-docs/pyspark-questions/401_pyspark_turn_off_force_utf8_encoding_in_pyspark.html)**

##参考
* [Python Unicode HOWTO](https://docs.python.org/3.8/howto/unicode.html#the-unicode-type)
* [Pragmatic Unicode](https://nedbatchelder.com/text/unipain.html)
* [codecs — Codec registry and base classes](https://docs.python.org/3/library/codecs.html#standard-encodings)